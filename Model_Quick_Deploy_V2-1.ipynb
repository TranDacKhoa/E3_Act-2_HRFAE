{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3tA2E7UY4YzW"
      },
      "source": [
        "# Install requirements"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zz7ghcWAatK5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d072ba3b-048c-479a-d892-a45fc53e938f"
      },
      "source": [
        "# Install requirements\n",
        "!pip install fastapi==0.68.1\n",
        "!pip install opencv-python==4.5.3.56\n",
        "# !pip install Pillow==8.3.2\n",
        "!pip install timm==1.\n",
        "!pip install python-multipart==0.0.5\n",
        "!pip install uvicorn==0.15.0"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting fastapi==0.68.1\n",
            "  Downloading fastapi-0.68.1-py3-none-any.whl (52 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/52.3 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m52.3/52.3 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting starlette==0.14.2 (from fastapi==0.68.1)\n",
            "  Downloading starlette-0.14.2-py3-none-any.whl (60 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/60.6 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.6/60.6 kB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pydantic!=1.7,!=1.7.1,!=1.7.2,!=1.7.3,!=1.8,!=1.8.1,<2.0.0,>=1.6.2 in /usr/local/lib/python3.10/dist-packages (from fastapi==0.68.1) (1.10.9)\n",
            "Requirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.7,!=1.7.1,!=1.7.2,!=1.7.3,!=1.8,!=1.8.1,<2.0.0,>=1.6.2->fastapi==0.68.1) (4.6.3)\n",
            "Installing collected packages: starlette, fastapi\n",
            "Successfully installed fastapi-0.68.1 starlette-0.14.2\n",
            "Collecting opencv-python==4.5.3.56\n",
            "  Downloading opencv-python-4.5.3.56.tar.gz (89.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m89.2/89.2 MB\u001b[0m \u001b[31m9.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
            "  \n",
            "  \u001b[31m×\u001b[0m \u001b[32mpip subprocess to install build dependencies\u001b[0m did not run successfully.\n",
            "  \u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
            "  \u001b[31m╰─>\u001b[0m See above for output.\n",
            "  \n",
            "  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25herror\n",
            "\u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
            "\n",
            "\u001b[31m×\u001b[0m \u001b[32mpip subprocess to install build dependencies\u001b[0m did not run successfully.\n",
            "\u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
            "\u001b[31m╰─>\u001b[0m See above for output.\n",
            "\n",
            "\u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
            "Collecting timm\n",
            "  Downloading timm-0.9.2-py3-none-any.whl (2.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m29.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: torch>=1.7 in /usr/local/lib/python3.10/dist-packages (from timm) (2.0.1+cu118)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (from timm) (0.15.2+cu118)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from timm) (6.0)\n",
            "Collecting huggingface-hub (from timm)\n",
            "  Downloading huggingface_hub-0.16.4-py3-none-any.whl (268 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m268.8/268.8 kB\u001b[0m \u001b[31m27.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting safetensors (from timm)\n",
            "  Downloading safetensors-0.3.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m66.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.7->timm) (3.12.2)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch>=1.7->timm) (4.6.3)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.7->timm) (1.11.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.7->timm) (3.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.7->timm) (3.1.2)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.7->timm) (2.0.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.7->timm) (3.25.2)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.7->timm) (16.0.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->timm) (2023.6.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->timm) (2.27.1)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->timm) (4.65.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->timm) (23.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision->timm) (1.22.4)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision->timm) (8.4.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.7->timm) (2.1.3)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->timm) (1.26.16)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->timm) (2023.5.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->timm) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->timm) (3.4)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.7->timm) (1.3.0)\n",
            "Installing collected packages: safetensors, huggingface-hub, timm\n",
            "Successfully installed huggingface-hub-0.16.4 safetensors-0.3.1 timm-0.9.2\n",
            "Collecting python-multipart==0.0.5\n",
            "  Downloading python-multipart-0.0.5.tar.gz (32 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: six>=1.4.0 in /usr/local/lib/python3.10/dist-packages (from python-multipart==0.0.5) (1.16.0)\n",
            "Building wheels for collected packages: python-multipart\n",
            "  Building wheel for python-multipart (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for python-multipart: filename=python_multipart-0.0.5-py3-none-any.whl size=31670 sha256=4c7beee92311e64f81bb1f5d37456785a59a14b7ec826e798c3093e0fef6f9b7\n",
            "  Stored in directory: /root/.cache/pip/wheels/ae/3f/03/fa4bd98cd7f4a25e63b6a0b61a7a8352e0f874cd9de1f3390d\n",
            "Successfully built python-multipart\n",
            "Installing collected packages: python-multipart\n",
            "Successfully installed python-multipart-0.0.5\n",
            "Collecting uvicorn==0.15.0\n",
            "  Downloading uvicorn-0.15.0-py3-none-any.whl (54 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.4/54.4 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting asgiref>=3.4.0 (from uvicorn==0.15.0)\n",
            "  Downloading asgiref-3.7.2-py3-none-any.whl (24 kB)\n",
            "Requirement already satisfied: click>=7.0 in /usr/local/lib/python3.10/dist-packages (from uvicorn==0.15.0) (8.1.3)\n",
            "Collecting h11>=0.8 (from uvicorn==0.15.0)\n",
            "  Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: typing-extensions>=4 in /usr/local/lib/python3.10/dist-packages (from asgiref>=3.4.0->uvicorn==0.15.0) (4.6.3)\n",
            "Installing collected packages: h11, asgiref, uvicorn\n",
            "Successfully installed asgiref-3.7.2 h11-0.14.0 uvicorn-0.15.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DokMreh-e2YC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3bd52d26-2333-4908-cb45-afbb15bfce0b"
      },
      "source": [
        "!pip install nest-asyncio"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nest-asyncio in /usr/local/lib/python3.10/dist-packages (1.5.6)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NYDZfIQ0ljYx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a15f3f7b-3726-4225-c1a8-9d4d26dc217e"
      },
      "source": [
        "!pip install pyngrok"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pyngrok\n",
            "  Downloading pyngrok-6.0.0.tar.gz (681 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/681.2 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m368.6/681.2 kB\u001b[0m \u001b[31m10.8 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m681.2/681.2 kB\u001b[0m \u001b[31m12.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.10/dist-packages (from pyngrok) (6.0)\n",
            "Building wheels for collected packages: pyngrok\n",
            "  Building wheel for pyngrok (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyngrok: filename=pyngrok-6.0.0-py3-none-any.whl size=19867 sha256=bd38909ae7d108ec6562d6ee1c83b7277f3146ae358f67a036b34615f9ce2316\n",
            "  Stored in directory: /root/.cache/pip/wheels/5c/42/78/0c3d438d7f5730451a25f7ac6cbf4391759d22a67576ed7c2c\n",
            "Successfully built pyngrok\n",
            "Installing collected packages: pyngrok\n",
            "Successfully installed pyngrok-6.0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import List\n",
        "\n",
        "from fastapi import FastAPI, File, UploadFile\n",
        "from fastapi.responses import HTMLResponse, StreamingResponse\n",
        "\n",
        "import cv2\n",
        "import io\n",
        "import numpy as np\n",
        "\n",
        "import torch\n",
        "import cv2\n",
        "from PIL import Image\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.backends.backend_agg import FigureCanvasAgg as FigureCanvas\n",
        "from matplotlib.figure import Figure"
      ],
      "metadata": {
        "id": "EIQ3BGWgNAe6"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## DPT Model"
      ],
      "metadata": {
        "id": "CBkSccNGQhE7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def load_model_dpt(model_type):\n",
        "  ## Load model\n",
        "\n",
        "  # MiDaS v3 - Large\n",
        "  # (highest accuracy, slowest inference speed)\n",
        "  # model_type = \"DPT_Large\"\n",
        "\n",
        "  # MiDaS v3 - Hybrid\n",
        "  # (medium accuracy, medium inference speed)\n",
        "  # model_type = \"DPT_Hybrid\"\n",
        "\n",
        "  # (lowest accuracy, highest inference speed)\n",
        "  # model_type = \"MiDaS_small\"  # MiDaS v2.1 - Small\n",
        "\n",
        "  midas = torch.hub.load(\"intel-isl/MiDaS\", model_type)\n",
        "\n",
        "  device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "  midas.to(device)\n",
        "  midas.eval()\n",
        "\n",
        "  return midas\n",
        "\n",
        "\n",
        "def pre_process_dpt(image, model_type):\n",
        "\n",
        "    midas_transforms = torch.hub.load(\"intel-isl/MiDaS\", \"transforms\")\n",
        "    if model_type == \"DPT_Large\" or model_type == \"DPT_Hybrid\":\n",
        "        transform = midas_transforms.dpt_transform\n",
        "    else:\n",
        "        transform = midas_transforms.small_transfor\n",
        "\n",
        "    # Load image\n",
        "    img = cv2.imdecode(np.frombuffer(image.file.read(),\n",
        "                                      np.uint8),\n",
        "                        cv2.IMREAD_COLOR)\n",
        "\n",
        "    # convert it to the correct format\n",
        "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "    # Transform it so that it can be used by the model\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    input_batch = transform(img).to(device)\n",
        "\n",
        "    # Return this image so it can be used in postprocessing\n",
        "    return input_batch, img\n",
        "\n",
        "def post_process_dpt(original, prediction):\n",
        "\n",
        "  prediction = torch.nn.functional.interpolate(\n",
        "                prediction.unsqueeze(1),\n",
        "                size=original.shape[:2],\n",
        "                mode=\"bicubic\",\n",
        "                align_corners=False,\n",
        "            ).squeeze()\n",
        "\n",
        "  output = prediction.cpu().numpy()\n",
        "  # Create a figure using matplotlib which super-imposes the original\n",
        "  # image and the prediction\n",
        "\n",
        "  fig = Figure()\n",
        "  canvas = FigureCanvas(fig)\n",
        "  ax = fig.gca()\n",
        "\n",
        "  # Render both images original as foreground\n",
        "  ax.imshow(original)\n",
        "  ax.imshow(output)\n",
        "\n",
        "  ax.axis(\"off\")\n",
        "  canvas.draw()\n",
        "\n",
        "  # Reshape output to be a numpy array\n",
        "  width, height = fig.get_size_inches() * fig.get_dpi()\n",
        "  width = int(width)\n",
        "  height = int(height)\n",
        "  output_image = np.frombuffer(canvas.tostring_rgb(),\n",
        "                                dtype='uint8').reshape(height, width, 3)\n",
        "\n",
        "  # Encode to png\n",
        "  res, im_png = cv2.imencode(\".png\", output_image)\n",
        "\n",
        "  return im_png\n",
        "\n"
      ],
      "metadata": {
        "id": "rsCQ-SqcLLVW"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cd4njq644OGL"
      },
      "source": [
        "# Setup the server"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LLQNYveOD3jr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "16b07252-74b1-4569-92d5-046e0048dd06"
      },
      "source": [
        "!git clone https://github.com/InterDigitalInc/HRFAE.git\n",
        "%cd HRFAE"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'HRFAE'...\n",
            "remote: Enumerating objects: 116, done.\u001b[K\n",
            "remote: Total 116 (delta 0), reused 0 (delta 0), pack-reused 116\u001b[K\n",
            "Receiving objects: 100% (116/116), 2.79 MiB | 12.91 MiB/s, done.\n",
            "Resolving deltas: 100% (42/42), done.\n",
            "/content/HRFAE\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NcsZC87Nab_x",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "20b149d5-cf4a-4039-f25e-2aafb4ce8a75"
      },
      "source": [
        "%cd logs/001\n",
        "!./download.sh\n",
        "%cd ./../.."
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/HRFAE/logs/001\n",
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100 49.2M  100 49.2M    0     0   563k      0  0:01:29  0:01:29 --:--:--  594k\n",
            "/content/HRFAE\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3FeQyYohljVu"
      },
      "source": [
        "import argparse\n",
        "import os\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.utils.data as data\n",
        "import yaml\n",
        "\n",
        "import matplotlib\n",
        "matplotlib.use('agg')\n",
        "from matplotlib import pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "from PIL import Image\n",
        "from torchvision import transforms, utils\n",
        "\n",
        "from datasets import *\n",
        "from nets import *\n",
        "from functions import *\n",
        "from trainer import *"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VVpLnIp3l3EX"
      },
      "source": [
        "parser = argparse.ArgumentParser()\n",
        "parser.add_argument('--config', type=str, default='001', help='path to the config file.')\n",
        "parser.add_argument('--vgg_model_path', type=str, default='./models/dex_imdb_wiki.caffemodel.pt', help='pretrained age classifier')\n",
        "parser.add_argument('--log_path', type=str, default='./logs/', help='log file path')\n",
        "parser.add_argument('--multigpu', type=bool, default=False, help='use multiple gpus')\n",
        "parser.add_argument('--checkpoint', type=str, default='', help='checkpoint file path')\n",
        "parser.add_argument('--img_path', type=str, default='./test/input/', help='test image path')\n",
        "parser.add_argument('--out_path', type=str, default='./test/output/', help='test output path')\n",
        "parser.add_argument('--target_age', type=int, default=55, help='Age transform target, interger value between 20 and 70')\n",
        "opts = parser.parse_known_args()[0]\n",
        "\n",
        "log_dir = os.path.join(opts.log_path, opts.config) + '/'\n",
        "if not os.path.exists(opts.out_path):\n",
        "    os.makedirs(opts.out_path)\n",
        "\n",
        "config = yaml.safe_load(open('./configs/' + opts.config + '.yaml', 'r'))\n",
        "img_size = (config['input_w'], config['input_h'])\n",
        "\n",
        "# Initialize trainer\n",
        "trainer = Trainer(config)\n",
        "device = torch.device('cuda')\n",
        "trainer.to(device)\n",
        "\n",
        "# Load pretrained model\n",
        "if opts.checkpoint:\n",
        "    trainer.load_checkpoint(opts.checkpoint)\n",
        "else:\n",
        "    trainer.load_checkpoint(log_dir + 'checkpoint')\n",
        "\n",
        "def preprocess(img_pil):\n",
        "    resize = transforms.Compose([\n",
        "            transforms.Resize(img_size),\n",
        "            transforms.ToTensor()\n",
        "            ])\n",
        "    normalize = transforms.Normalize(mean=[0.48501961, 0.45795686, 0.40760392], std=[1,1,1])\n",
        "\n",
        "    img_np = np.array(img_pil)\n",
        "    img = resize(img_pil)\n",
        "    if img.size(0) == 1:\n",
        "        img = torch.cat((img, img, img), dim = 0)\n",
        "    img = normalize(img)\n",
        "    return img"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def is_image(variable):\n",
        "    try:\n",
        "        Image.open(variable)\n",
        "        return True\n",
        "    except (IOError, OSError):\n",
        "        return False"
      ],
      "metadata": {
        "id": "kEN0gFxLHYJY"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ANG-77q5arAo"
      },
      "source": [
        "import base64\n",
        "\n",
        "# Code from: https://fastapi.tiangolo.com/tutorial/request-files/\n",
        "app = FastAPI()\n",
        "\n",
        "\n",
        "@app.post(\"/uploadfiles/\")\n",
        "async def create_upload_files(files: List[UploadFile] = File(...)):\n",
        "    \"\"\" Create API endpoint to send image to and specify\n",
        "     what type of file it'll take\n",
        "\n",
        "    :param files: Get image files, defaults to File(...)\n",
        "    :type files: List[UploadFile], optional\n",
        "    :return: A list of png images\n",
        "    :rtype: list(bytes)\n",
        "    \"\"\"\n",
        "\n",
        "        # Set target age\n",
        "    target_age = 65\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for file in files:\n",
        "            contents = await file.read()  # Đọc nội dung của tệp ảnh\n",
        "            image = Image.open(io.BytesIO(contents))  # Tạo đối tượng Image từ nội dung\n",
        "\n",
        "            img_name = file.filename\n",
        "            if not img_name.endswith(('png', 'jpg', 'PNG', 'JPG')):\n",
        "                print('File ignored: ' + img_name)\n",
        "                continue\n",
        "            image_A = preprocess(image)\n",
        "            image_A = image_A.unsqueeze(0).to(device)\n",
        "\n",
        "            age_modif = torch.tensor(target_age).unsqueeze(0).to(device)\n",
        "            image_A_modif = trainer.test_eval(image_A, age_modif, target_age=target_age, hist_trans=True)\n",
        "            utils.save_image(clip_img(image_A_modif), opts.out_path + img_name.split('.')[0] + '_age_' + str(target_age) + '.jpg')\n",
        "\n",
        "            # Plot manipulated image\n",
        "            print(opts.out_path + img_name.split('.')[0] + '_age_' + str(target_age) + '.jpg')\n",
        "\n",
        "            img_show = Image.open(opts.out_path + img_name.split('.')[0] + '_age_' + str(target_age) + '.jpg')\n",
        "\n",
        "            img_out = np.array(img_show)\n",
        "            plt.axis('off')\n",
        "            plt.imshow(img_out)\n",
        "            plt.show()\n",
        "\n",
        "            # Chuyển đổi hình ảnh thành chuỗi byte\n",
        "            byte_stream = io.BytesIO()\n",
        "            img_show.save(byte_stream, format='JPEG')\n",
        "            byte_stream.seek(0)\n",
        "\n",
        "            # # Trả về hình ảnh dưới dạng streaming response\n",
        "            # return StreamingResponse(byte_stream, media_type=\"image/JPEG\")\n",
        "\n",
        "            # Tạo nội dung HTML tùy chỉnh\n",
        "            content = f\"\"\"\n",
        "                <html>\n",
        "                <head>\n",
        "                    <title>Uploaded Image</title>\n",
        "                    <style>\n",
        "                        body {{\n",
        "                            padding: 20px;\n",
        "                            text-align: center;\n",
        "                        }}\n",
        "                        h1 {{\n",
        "                            color: #333;\n",
        "                        }}\n",
        "                        .image-container {{\n",
        "                            margin-top: 20px;\n",
        "                        }}\n",
        "                        .image-container img {{\n",
        "                            max-width: 100%;\n",
        "                            height: auto;\n",
        "                        }}\n",
        "                    </style>\n",
        "                </head>\n",
        "                <body>\n",
        "                    <h1>Uploaded Image</h1>\n",
        "                    <div class=\"image-container\">\n",
        "                        <img src=\"data:image/jpeg;base64,{base64.b64encode(byte_stream.getvalue()).decode('utf-8')}\">\n",
        "                    </div>\n",
        "                </body>\n",
        "                </html>\n",
        "            \"\"\"\n",
        "\n",
        "            # Trả về nội dung HTML tùy chỉnh\n",
        "            return HTMLResponse(content=content)\n",
        "\n"
      ],
      "execution_count": 136,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@app.get(\"/\")\n",
        "async def main():\n",
        "    \"\"\"Create a basic home page to upload a file\n",
        "\n",
        "    :return: HTML for homepage\n",
        "    :rtype: HTMLResponse\n",
        "    \"\"\"\n",
        "\n",
        "    content = \"\"\"<body>\n",
        "          <h3>Upload an image to get it's depth map from the MiDaS model</h3>\n",
        "          <form action=\"/uploadfiles/\" enctype=\"multipart/form-data\" method=\"post\">\n",
        "              <input name=\"files\" type=\"file\" multiple>\n",
        "              <input type=\"submit\">\n",
        "          </form>\n",
        "      </body>\n",
        "      \"\"\"\n",
        "    return HTMLResponse(content=content)"
      ],
      "metadata": {
        "id": "jBN39acsRa_g"
      },
      "execution_count": 137,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EBbE8OI8fOVa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "53e41d29-32d1-4a00-c466-caed400d92b8"
      },
      "source": [
        "auth_token = \"2SNVvi3DMeCAGfkN1l6lRbqhCg5_5GC4C21zFzmGGvGTDhv2w\" #@param {type:\"string\"}\n",
        "# Since we can't access Colab notebooks IP directly we'll use\n",
        "# ngrok to create a public URL for the server via a tunnel\n",
        "\n",
        "# Authenticate ngrok\n",
        "# https://dashboard.ngrok.com/signup\n",
        "# Then go to the \"Your Authtoken\" tab in the sidebar and copy the API key\n",
        "import os\n",
        "os.system(f\"ngrok authtoken {auth_token}\")"
      ],
      "execution_count": 138,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {},
          "execution_count": 138
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IHrh3rOnmtuA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ef47eea4-16e9-4102-abd4-41a5d949d574"
      },
      "source": [
        "\n",
        "from pyngrok import ngrok\n",
        "\n",
        "# Create tunnel\n",
        "public_url = ngrok.connect(8000,bind_tls=True)"
      ],
      "execution_count": 139,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:pyngrok.process.ngrok:t=2023-07-10T14:48:14+0000 lvl=warn msg=\"ngrok config file found at legacy location, move to XDG location\" xdg_path=/root/.config/ngrok/ngrok.yml legacy_path=/root/.ngrok2/ngrok.yml\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-hqiEAo5PpLu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "148a9e2a-e1aa-4ea6-cd8b-184be547cffe"
      },
      "source": [
        "# Check if it exists\n",
        "!ps aux | grep ngrok"
      ],
      "execution_count": 140,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root       20768  0.0  0.2 736192 27824 ?        Sl   14:48   0:00 /usr/local/lib/python3.10/dist-packages/pyngrok/bin/ngrok start --none --log=stdout\n",
            "root       20777  0.0  0.0   6904  3216 ?        S    14:48   0:00 /bin/bash -c ps aux | grep ngrok\n",
            "root       20779  0.0  0.0   6444   660 ?        S    14:48   0:00 grep ngrok\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6NVUy4iL43jg"
      },
      "source": [
        "# Make magic happen"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ggazuIY0auLI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "968410ef-e6c7-442c-cea3-dfafceb7c27d"
      },
      "source": [
        "import nest_asyncio\n",
        "\n",
        "# Allow for asyncio to work within the Jupyter notebook cell\n",
        "nest_asyncio.apply()\n",
        "\n",
        "import uvicorn\n",
        "\n",
        "# Run the FastAPI app using uvicorn\n",
        "print(public_url)\n",
        "uvicorn.run(app)"
      ],
      "execution_count": 141,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NgrokTunnel: \"https://e616-34-67-182-18.ngrok-free.app\" -> \"http://localhost:8000\"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:     Started server process [173]\n",
            "INFO:uvicorn.error:Started server process [173]\n",
            "INFO:     Waiting for application startup.\n",
            "INFO:uvicorn.error:Waiting for application startup.\n",
            "INFO:     Application startup complete.\n",
            "INFO:uvicorn.error:Application startup complete.\n",
            "INFO:     Uvicorn running on http://127.0.0.1:8000 (Press CTRL+C to quit)\n",
            "INFO:uvicorn.error:Uvicorn running on http://127.0.0.1:8000 (Press CTRL+C to quit)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:     2402:800:63b8:c917:84f2:2bf1:53d2:668c:0 - \"GET / HTTP/1.1\" 200 OK\n",
            "INFO:     2402:800:63b8:c917:84f2:2bf1:53d2:668c:0 - \"GET /Thay%20%C4%91%E1%BB%95i%20k%C3%ADch%20th%C6%B0%E1%BB%9Bc%20nhi%E1%BB%81u%20%E1%BA%A3nh%20c%C3%B9ng%20l%C3%BAc%21_files/plupload.full.min.js.ta%CC%89i%20xu%C3%B4%CC%81ng HTTP/1.1\" 404 Not Found\n",
            "INFO:     2402:800:63b8:c917:84f2:2bf1:53d2:668c:0 - \"GET /Thay%20%C4%91%E1%BB%95i%20k%C3%ADch%20th%C6%B0%E1%BB%9Bc%20nhi%E1%BB%81u%20%E1%BA%A3nh%20c%C3%B9ng%20l%C3%BAc%21_files/app.b7548e8.css HTTP/1.1\" 404 Not Found\n",
            "INFO:     2402:800:63b8:c917:84f2:2bf1:53d2:668c:0 - \"GET /Thay%20%C4%91%E1%BB%95i%20k%C3%ADch%20th%C6%B0%E1%BB%9Bc%20nhi%E1%BB%81u%20%E1%BA%A3nh%20c%C3%B9ng%20l%C3%BAc%21_files/esp.js.ta%CC%89i%20xu%C3%B4%CC%81ng HTTP/1.1\" 404 Not Found\n",
            "INFO:     2402:800:63b8:c917:84f2:2bf1:53d2:668c:0 - \"GET /Thay%20%C4%91%E1%BB%95i%20k%C3%ADch%20th%C6%B0%E1%BB%9Bc%20nhi%E1%BB%81u%20%E1%BA%A3nh%20c%C3%B9ng%20l%C3%BAc%21_files/f%282%29.txt HTTP/1.1\" 404 Not Found\n",
            "INFO:     2402:800:63b8:c917:84f2:2bf1:53d2:668c:0 - \"GET /Thay%20%C4%91%E1%BB%95i%20k%C3%ADch%20th%C6%B0%E1%BB%9Bc%20nhi%E1%BB%81u%20%E1%BA%A3nh%20c%C3%B9ng%20l%C3%BAc%21_files/publishertag.ids.js.ta%CC%89i%20xu%C3%B4%CC%81ng HTTP/1.1\" 404 Not Found\n",
            "INFO:     2402:800:63b8:c917:84f2:2bf1:53d2:668c:0 - \"GET /Thay%20%C4%91%E1%BB%95i%20k%C3%ADch%20th%C6%B0%E1%BB%9Bc%20nhi%E1%BB%81u%20%E1%BA%A3nh%20c%C3%B9ng%20l%C3%BAc%21_files/ev.min.js.ta%CC%89i%20xu%C3%B4%CC%81ng HTTP/1.1\" 404 Not Found\n",
            "INFO:     2402:800:63b8:c917:84f2:2bf1:53d2:668c:0 - \"GET /Thay%20%C4%91%E1%BB%95i%20k%C3%ADch%20th%C6%B0%E1%BB%9Bc%20nhi%E1%BB%81u%20%E1%BA%A3nh%20c%C3%B9ng%20l%C3%BAc%21_files/f.txt HTTP/1.1\" 404 Not Found\n",
            "INFO:     2402:800:63b8:c917:84f2:2bf1:53d2:668c:0 - \"GET /Thay%20%C4%91%E1%BB%95i%20k%C3%ADch%20th%C6%B0%E1%BB%9Bc%20nhi%E1%BB%81u%20%E1%BA%A3nh%20c%C3%B9ng%20l%C3%BAc%21_files/js HTTP/1.1\" 404 Not Found\n",
            "INFO:     2402:800:63b8:c917:84f2:2bf1:53d2:668c:0 - \"GET /Thay%20%C4%91%E1%BB%95i%20k%C3%ADch%20th%C6%B0%E1%BB%9Bc%20nhi%E1%BB%81u%20%E1%BA%A3nh%20c%C3%B9ng%20l%C3%BAc%21_files/plupload.full.min.js.ta%CC%89i%20xu%C3%B4%CC%81ng HTTP/1.1\" 404 Not Found\n",
            "INFO:     2402:800:63b8:c917:84f2:2bf1:53d2:668c:0 - \"GET /Thay%20%C4%91%E1%BB%95i%20k%C3%ADch%20th%C6%B0%E1%BB%9Bc%20nhi%E1%BB%81u%20%E1%BA%A3nh%20c%C3%B9ng%20l%C3%BAc%21_files/preload.svg HTTP/1.1\" 404 Not Found\n",
            "INFO:     2402:800:63b8:c917:84f2:2bf1:53d2:668c:0 - \"GET /Thay%20%C4%91%E1%BB%95i%20k%C3%ADch%20th%C6%B0%E1%BB%9Bc%20nhi%E1%BB%81u%20%E1%BA%A3nh%20c%C3%B9ng%20l%C3%BAc%21_files/iloveimg.svg HTTP/1.1\" 404 Not Found\n",
            "INFO:     2402:800:63b8:c917:84f2:2bf1:53d2:668c:0 - \"GET /Thay%20%C4%91%E1%BB%95i%20k%C3%ADch%20th%C6%B0%E1%BB%9Bc%20nhi%E1%BB%81u%20%E1%BA%A3nh%20c%C3%B9ng%20l%C3%BAc%21_files/f%281%29.txt HTTP/1.1\" 404 Not Found\n",
            "INFO:     2402:800:63b8:c917:84f2:2bf1:53d2:668c:0 - \"GET /js/ev.min.js?a=1.2 HTTP/1.1\" 404 Not Found\n",
            "INFO:     2402:800:63b8:c917:84f2:2bf1:53d2:668c:0 - \"GET /Thay%20%C4%91%E1%BB%95i%20k%C3%ADch%20th%C6%B0%E1%BB%9Bc%20nhi%E1%BB%81u%20%E1%BA%A3nh%20c%C3%B9ng%20l%C3%BAc%21_files/app.b7548e8.js.ta%CC%89i%20xu%C3%B4%CC%81ng HTTP/1.1\" 404 Not Found\n",
            "INFO:     2402:800:63b8:c917:84f2:2bf1:53d2:668c:0 - \"GET /Thay%20%C4%91%E1%BB%95i%20k%C3%ADch%20th%C6%B0%E1%BB%9Bc%20nhi%E1%BB%81u%20%E1%BA%A3nh%20c%C3%B9ng%20l%C3%BAc%21_files/container.html HTTP/1.1\" 404 Not Found\n",
            "INFO:     2402:800:63b8:c917:84f2:2bf1:53d2:668c:0 - \"GET /Thay%20%C4%91%E1%BB%95i%20k%C3%ADch%20th%C6%B0%E1%BB%9Bc%20nhi%E1%BB%81u%20%E1%BA%A3nh%20c%C3%B9ng%20l%C3%BAc%21_files/syncframe.html HTTP/1.1\" 404 Not Found\n",
            "INFO:     2402:800:63b8:c917:84f2:2bf1:53d2:668c:0 - \"GET /Thay%20%C4%91%E1%BB%95i%20k%C3%ADch%20th%C6%B0%E1%BB%9Bc%20nhi%E1%BB%81u%20%E1%BA%A3nh%20c%C3%B9ng%20l%C3%BAc%21_files/pd.html HTTP/1.1\" 404 Not Found\n",
            "INFO:     2402:800:63b8:c917:84f2:2bf1:53d2:668c:0 - \"GET /Thay%20%C4%91%E1%BB%95i%20k%C3%ADch%20th%C6%B0%E1%BB%9Bc%20nhi%E1%BB%81u%20%E1%BA%A3nh%20c%C3%B9ng%20l%C3%BAc%21_files/saved_resource.html HTTP/1.1\" 404 Not Found\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:     Shutting down\n",
            "INFO:uvicorn.error:Shutting down\n",
            "INFO:     Waiting for application shutdown.\n",
            "INFO:uvicorn.error:Waiting for application shutdown.\n",
            "INFO:     Application shutdown complete.\n",
            "INFO:uvicorn.error:Application shutdown complete.\n",
            "INFO:     Finished server process [173]\n",
            "INFO:uvicorn.error:Finished server process [173]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SAgM7AQHgh8N"
      },
      "source": [
        "# Kill tunnel\n",
        "ngrok.disconnect(public_url=public_url)"
      ],
      "execution_count": 142,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "30W96DhCN05k"
      },
      "execution_count": 142,
      "outputs": []
    }
  ]
}